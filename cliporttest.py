# -*- coding: utf-8 -*-
"""Cliport

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sZSnNFdNLMPagFdS88Iy6HeW5HLqxtxz
"""

!git clone https://github.com/cliport/cliport.git

!pip install --upgrade pip

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/cliport
# %env CLIPORT_ROOT=/content/cliport/
import torch
import numpy as np
torch.manual_seed(120)
np.random.seed(120)

!pip install -r requirements.txt

!pip install torchtext==0.8.1

!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html

!pip install captum

!python setup.py develop

from google.colab import drive
drive.mount('/content/drive/')
!unzip "/content/drive/My Drive/Research/cliport_quickstart.zip" -d "/content/cliport/"

!unzip "/content/drive/My Drive/Research/google.zip" -d "/content/cliport/cliport/environments/assets/"

import shutil
shutil.rmtree('/content/cliport/data/stack-block-pyramid-seq-unseen-colors-test')

!python cliport/demos.py n=1 \
                        task=stack-block-pyramid-seq-unseen-colors \
                        mode=test

'''!python cliport/eval.py model_task=multi-language-conditioned \
                       eval_task=stack-block-pyramid-seq-seen-colors \
                       agent=cliport \
                       mode=test \
                       n_demos=5 \
                       train_demos=1000 \
                       exp_folder=cliport_quickstart \
                       checkpoint_type=test_best \
                       update_results=True \
                       disp=False'''

# Commented out IPython magic to ensure Python compatibility.
# set GPU
# %env CUDA_VISIBLE_DEVICES=0

import os
import sys
import json

import numpy as np
from cliport import tasks
from cliport import agents
from cliport.utils import utils

import torch
import cv2
from cliport.dataset import RavensDataset
from cliport.environments.environment import Environment

# %matplotlib inline
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap

from captum.attr import IntegratedGradients
from captum.attr import GradientShap
from captum.attr import Occlusion
from captum.attr import NoiseTunnel
from captum.attr import visualization as viz
from captum.attr import LayerGradCam

train_demos = 1000 # number training demonstrations used to train agent
n_eval = 1 # number of evaluation instances
mode = 'test' # val or test

agent_name = 'cliport'
model_task = 'multi-language-conditioned' # multi-task agent conditioned with language goals

model_folder = 'cliport_quickstart' # path to pre-trained checkpoint
ckpt_name = 'steps=400000-val_loss=0.00014655.ckpt' # name of checkpoint to load

draw_grasp_lines = True
affordance_heatmap_scale = 100 #30

### Uncomment the task you want to evaluate on ###
#eval_task = 'align-rope'
# eval_task = 'assembling-kits-seq-seen-colors'
# eval_task = 'assembling-kits-seq-unseen-colors'
# eval_task = 'packing-shapes'
# eval_task = 'packing-boxes-pairs-seen-colors'
# eval_task = 'packing-boxes-pairs-unseen-colors'
#eval_task = 'packing-seen-google-objects-seq'
#eval_task = 'packing-unseen-google-objects-seq'
# eval_task = 'packing-seen-google-objects-group'
# eval_task = 'packing-unseen-google-objects-group'
# eval_task = 'put-block-in-bowl-seen-colors'
# eval_task = 'put-block-in-bowl-unseen-colors'
#eval_task = 'stack-block-pyramid-seq-seen-colors'
eval_task = 'stack-block-pyramid-seq-unseen-colors'
# eval_task = 'separating-piles-seen-colors'
#eval_task = 'separating-piles-unseen-colors'
# eval_task = 'towers-of-hanoi-seq-seen-colors'
# eval_task = 'towers-of-hanoi-seq-unseen-colors'

root_dir = os.environ['CLIPORT_ROOT']
assets_root = os.path.join(root_dir, 'cliport/environments/assets/')
config_file = 'eval.yaml' 

vcfg = utils.load_hydra_config(os.path.join(root_dir, f'cliport/cfg/{config_file}'))
vcfg['data_dir'] = os.path.join(root_dir, 'data')
vcfg['mode'] = mode

vcfg['model_task'] = model_task
vcfg['eval_task'] = eval_task
vcfg['agent'] = agent_name

# Model and training config paths
model_path = os.path.join(root_dir, model_folder)
vcfg['train_config'] = f"{model_path}/{vcfg['model_task']}-{vcfg['agent']}-n{train_demos}-train/.hydra/config.yaml"
vcfg['model_path'] = f"{model_path}/{vcfg['model_task']}-{vcfg['agent']}-n{train_demos}-train/checkpoints/"

tcfg = utils.load_hydra_config(vcfg['train_config'])

# Load dataset
ds = RavensDataset(os.path.join(vcfg['data_dir'], f'{vcfg["eval_task"]}-{vcfg["mode"]}'), 
                   tcfg, 
                   n_demos=n_eval,
                   augment=False)

eval_run = 0
name = '{}-{}-{}-{}'.format(vcfg['eval_task'], vcfg['agent'], n_eval, eval_run)
print(f'\nEval ID: {name}\n')

# Initialize agent
utils.set_seed(eval_run, torch=True)
agent = agents.names[vcfg['agent']](name, tcfg, None, ds)

# Load checkpoint
ckpt_path = os.path.join(vcfg['model_path'], ckpt_name)
print(f'\nLoading checkpoint: {ckpt_path}')
agent.load(ckpt_path)

env = Environment(
    assets_root,
    disp=False,
    shared_memory=False,
    hz=480,
    record_cfg=False
)

device = torch.device('cuda:0')

#print(agent.transport.kernel_shape)
#print(agent.transport.in_shape)
print(agent.attention.stream_fcn)
with torch.no_grad():
    torch.cuda.empty_cache()

from captum.attr._utils.attribution import LayerAttribution
episode = 0
num_eval_instances = min(n_eval, ds.n_episodes)
from PIL import Image




for i in range(num_eval_instances):
    print(f'\nEvaluation Instance: {i + 1}/{num_eval_instances}')
    
    # Load episode
    episode, seed = ds.load(i)
    print(seed)
    goal = episode[-1]
    total_reward = 0
    np.random.seed(seed)

    # Set task
    task_name = vcfg['eval_task']
    task = tasks.names[task_name]()
    task.mode = mode
    
    # Set environment
    env.seed(seed)
    env.set_task(task)
    obs = env.reset()
    info = env.info
    reward = 0
    print(task.lang_template)
    
    step = 0
    done = False

    
    # Rollout
    while (step <= task.max_steps) and not done:
        print(f"Step: {step} ({task.max_steps} max)")
        
        # Get batch
        if step == task.max_steps-1:
            batch = ds.process_goal((obs, None, reward, info), perturb_params=None)
        else:
            batch = ds.process_sample((obs, None, reward, info), augment=False)

        fig, axs = plt.subplots(3, 2, figsize=(13, 7))
        
        # Get color and depth inputs
        img = batch['img']
        img = torch.from_numpy(img)
        color = np.uint8(img.detach().cpu().numpy())[:,:,:3]



        color = color.transpose(1,0,2)
        depth = np.array(img.detach().cpu().numpy())[:,:,3]
        depth = depth.transpose(1,0)

        test = ds.get_image(obs)
        test = Image.fromarray(color, 'RGB')
        display(test)
        
        # Display input color
        axs[0,0].imshow(color)
        axs[0,0].axes.xaxis.set_visible(False)
        axs[0,0].axes.yaxis.set_visible(False)
        axs[0,0].set_title('Input RGB')
        
        # Display input depth
        axs[0,1].imshow(depth)
        axs[0,1].axes.xaxis.set_visible(False)
        axs[0,1].axes.yaxis.set_visible(False)        
        axs[0,1].set_title('Input Depth')
        
        # Display predicted pick affordance
        axs[1,0].imshow(color)
        axs[1,0].axes.xaxis.set_visible(False)
        axs[1,0].axes.yaxis.set_visible(False)
        axs[1,0].set_title('Pick Affordance')
        
        # Display predicted place affordance
        axs[1,1].imshow(color)
        axs[1,1].axes.xaxis.set_visible(False)
        axs[1,1].axes.yaxis.set_visible(False)
        axs[1,1].set_title('Place Affordance')
        
        # Get action predictions
        l = str(info['lang_goal'])

        
        #CAPTUM
        img = agent.test_ds.get_image(obs)
        lang_goal = info['lang_goal']
        

        cap_img = torch.from_numpy(img).to(dtype=torch.float, device=agent.device)

        


        def unravel_indices(indices, shape):
          """Converts flat indices into unraveled coordinates in a target shape.

            Args:
              indices: A tensor of (flat) indices, (*, N).
              shape: The targeted shape, (D,).

            Returns:
              The unraveled coordinates, (*, N, D).
          """

          coord = []

          for dim in reversed(shape):
            coord.append(indices % dim)
            indices = indices // dim

          coord = torch.stack(coord[::-1], dim=-1)

          return coord


        def agg_segmentation_wrapper(cap_input):

          pick_inp = {'inp_img':cap_input, 'lang_goal': lang_goal}

          
          pick_conf = agent.attn_forward(pick_inp)

          
          #pick_conf = pick_conf.detach().cpu().numpy()
          
          
          argmax = torch.argmax(pick_conf)

          
          
          argmax = unravel_indices(argmax, shape=pick_conf.shape)
          p0_pix = argmax[:2]


          
          p0_theta = argmax[2] * (2 * np.pi / pick_conf.shape[2])

          #p0_pix = torch.reshape(p0_pix, (1,2))
          out = pick_conf[p0_pix[0], p0_pix[1], :]

          

          

          return out

        def agg_segmentation_wrapper_transport(cap_input):
          pick_inp = {'inp_img':img, 'lang_goal': lang_goal}

          
          pick_conf = agent.attn_forward(pick_inp)

          pick_conf = pick_conf.detach().cpu().numpy()
          argmax = np.argmax(pick_conf)
          argmax = np.unravel_index(argmax, shape=pick_conf.shape)
          p0_pix = argmax[:2]
          p0_theta = argmax[2] * (2 * np.pi / pick_conf.shape[2])

          place_inp = {'inp_img': cap_input, 'p0': p0_pix, 'lang_goal': lang_goal}
          place_conf = agent.trans_forward(place_inp)
          place_conf = place_conf.permute(1, 2, 0)
          argmax = torch.argmax(place_conf)
          argmax = unravel_indices(argmax, shape=place_conf.shape)
          p1_pix = argmax[:2]
          p1_theta = argmax[2] * (2 * np.pi / place_conf.shape[2])

          out = place_conf[p0_pix[0], p0_pix[1], :]


          return out



        
        #integrated_gradients = IntegratedGradients(agg_segmentation_wrapper)
        #attributions_ig = integrated_gradients.attribute(cap_img, internal_batch_size=2)

        integrated_gradients = IntegratedGradients(agg_segmentation_wrapper_transport)
        attributions_ig = integrated_gradients.attribute(cap_img, internal_batch_size=2)


        '''layer_gradcam = LayerGradCam(agg_segmentation_wrapper, agent.attention.attn_stream_one.conv2)
        attributions_lgc = layer_gradcam.attribute(cap_img)
        print(attributions_lgc.shape)
        _ = viz.visualize_image_attr(attributions_lgc[0].cpu().permute(1,2,0).detach().numpy(), sign='all', title = 'Semantic stream first conv')
        upsamp_attr_lgc = LayerAttribution.interpolate(attributions_lgc, cap_img.shape[:2])
        _ = viz.visualize_image_attr_multiple(upsamp_attr_lgc[0].cpu().permute(2,1,0).detach().numpy(),
                                      np.transpose(img[:,:,:3], (1,0,2)),
                                      ["original_image","blended_heat_map","masked_image"],
                                      ["all","positive","positive"],
                                      show_colorbar=True,
                                      titles=["Original", "Positive Attribution", "Masked"],
                                      fig_size=(18, 6))'''


        default_cmap = LinearSegmentedColormap.from_list('custom blue', 
                                                 [(0, '#ffffff'),
                                                  (0.25, '#000000'),
                                                  (1, '#000000')], N=256)
        
        _ = viz.visualize_image_attr(attributions_ig.cpu().detach().numpy()[:,:,:3].transpose(1,0,2),
                             img[:,:,:3].transpose(1,0,2),
                             method='heat_map',
                             cmap=default_cmap,
                             show_colorbar=True,
                             sign='positive',
                             outlier_perc=1)
        _ = viz.visualize_image_attr(attributions_ig.cpu().detach().numpy()[:,:,:3].transpose(1,0,2), sign="all")






        #CAPTUM_TEXT


       



        act = agent.act(obs, info, goal=None)
        pick, place = act['pick'], act['place']


        
        # Visualize pick affordance
        pick_inp = {'inp_img': batch['img'], 'lang_goal': l}
        pick_conf = agent.attn_forward(pick_inp)
        logits = pick_conf.detach().cpu().numpy()

        pick_conf = pick_conf.detach().cpu().numpy()
        argmax = np.argmax(pick_conf)
        argmax = np.unravel_index(argmax, shape=pick_conf.shape)
        p0 = argmax[:2]
        p0_theta = (argmax[2] * (2 * np.pi / pick_conf.shape[2])) * -1.0
    
        line_len = 30
        pick0 = (pick[0] + line_len/2.0 * np.sin(p0_theta), pick[1] + line_len/2.0 * np.cos(p0_theta))
        pick1 = (pick[0] - line_len/2.0 * np.sin(p0_theta), pick[1] - line_len/2.0 * np.cos(p0_theta))

        if draw_grasp_lines:
            axs[1,0].plot((pick1[0], pick0[0]), (pick1[1], pick0[1]), color='r', linewidth=1)
        
        # Visualize place affordance
        place_inp = {'inp_img': batch['img'], 'p0': pick, 'lang_goal': l}
        place_conf = agent.trans_forward(place_inp)

        place_conf = place_conf.permute(1, 2, 0)
        place_conf = place_conf.detach().cpu().numpy()
        argmax = np.argmax(place_conf)
        argmax = np.unravel_index(argmax, shape=place_conf.shape)
        p1_pix = argmax[:2]
        p1_theta = (argmax[2] * (2 * np.pi / place_conf.shape[2]) + p0_theta) * -1.0
        
        line_len = 30
        place0 = (place[0] + line_len/2.0 * np.sin(p1_theta), place[1] + line_len/2.0 * np.cos(p1_theta))
        place1 = (place[0] - line_len/2.0 * np.sin(p1_theta), place[1] - line_len/2.0 * np.cos(p1_theta))

        if draw_grasp_lines:
            axs[1,1].plot((place1[0], place0[0]), (place1[1], place0[1]), color='g', linewidth=1)
        
        # Overlay affordances on RGB input
        pick_logits_disp = np.uint8(logits * 255 * affordance_heatmap_scale).transpose(1,0,2)
        place_logits_disp = np.uint8(np.sum(place_conf, axis=2)[:,:,None] * 255 * affordance_heatmap_scale).transpose(1,0,2)    

        pick_logits_disp_masked = np.ma.masked_where(pick_logits_disp < 0, pick_logits_disp)
        place_logits_disp_masked = np.ma.masked_where(place_logits_disp < 0, place_logits_disp)
        pick_logits_disp_masked = pick_logits_disp_masked.squeeze()
        place_logits_disp_masked = place_logits_disp_masked.squeeze()

        axs[1][0].imshow(pick_logits_disp_masked, alpha=0.75)
        axs[1][1].imshow(place_logits_disp_masked, cmap='viridis', alpha=0.75)
        
        print(f"Lang Goal: {str(info['lang_goal'])}")
        plt.show()
        
        # Act with the predicted actions
        obs, reward, done, info = env.step(act)
        step += 1
        
    if done:
        print("Done. Success.")
    else:
        print("Max steps reached. Task failed.")

a = np.asarray([255.0 / 255.0, 87.0 / 255.0, 89.0 / 255.0])
b = np.asarray([176.0 / 255.0, 122.0 / 255.0, 161.0 / 255.0])
c = np.linspace(a, b, 5)
print(c)

